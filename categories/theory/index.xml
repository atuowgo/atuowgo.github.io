<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>理论 - 分类 - 程序猿二腊</title>
        <link>https://atuowgo.github.io/categories/theory/</link>
        <description>理论 - 分类 - 程序猿二腊</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Tue, 21 Feb 2023 01:36:54 -0700</lastBuildDate><atom:link href="https://atuowgo.github.io/categories/theory/" rel="self" type="application/rss+xml" /><item>
    <title>深度学习:神经网络</title>
    <link>https://atuowgo.github.io/deeplearning_02_neural_network/</link>
    <pubDate>Tue, 21 Feb 2023 01:36:54 -0700</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/deeplearning_02_neural_network/</guid>
    <description><![CDATA[神经网络 接着上节感知机后，引入神经网络
1. 激活函数 上一节介绍了两个输入信号的感知机的例子，以及用数学公式表示的例子，具体可以见上节内容,感知机。
可以将上节感知机的公式调整为
$$ y = h(b+w_1x_1+w_2x_2) (1.1) $$
其中函数$h(x)$的定义如下 $$ h(x) = \begin{cases} 0 (x \le 0) \ 1 (x \gt 0) \end{cases} (1.2)$$
结合示例如下：
其中函数$h(x)$称之为激活函数，该函数会将输入信号的总和转换为输出信号。而灰色的输入信号是为了体现偏执$b$专门加上去的，主要是为了方便理解。
1.1 阶跃函数 上面(1.2)就是阶跃函数，它的值呈阶段式变化，所以称为阶跃函数。
1.2 sigmoid函数 sigmoid函数如下
$$ h(x) = \displaystyle\frac{1}{1+e^{-x}} (1.3)$$
如下，sigmoid函数是一条平滑的曲线，输出随着输入发生连续性的变化，相对于阶跃函数，具有平滑性，在深度网络的学习中具有重要意义，能够增加表达能力。
另一个不同点是，相对于阶跃只能返回0或1，sigmoid函数可以返回0.731&hellip;,0.880&hellip;等实物。也就是说，感知机中神经元之间流动的是0和1的二元信号，而神经网络中流动的是连续的实数值信号。
虽然阶跃函数和sigmoid函数在平滑性上有差异，但二者还是有相同的点，当输入值越小，输出越接近0，输入值越大，输出越接近1.也就是说，当输入信号为重要信息时，阶跃函数和sigmoid都会输出较大值，当输入信号为不重要的信息时，二者都会输出较小的值。还有一个共同点，不管输入 信号多小，或者有多大，输出信号的值都在0和1之间。另外一个相同点是，它两都是非线性函数。
神经网络的激活函数必须使用非线性函数，如果使用线性函数作为激活函数的话，加深神经网络的层数就没有意义了。以激活函数$h(x)=cx$举例，运算3层后结果为$y(x)=h(h(h(x)))$，即$y(x)=c^3x$等价于$y(x)=ax$，即对于线性函数来说，不管加多少层，总是存在与之等效的&quot;无隐藏层的神经网络&quot;。
1.3 ReLU函数 如下，ReLU函数在输出大于0时，直接输出该值，在输出小于等于0时，输出0.
$$ h(x) = \begin{cases} 0 (x \le 0) \ x (x \gt 0) \end{cases} (1.4)$$
2. 神经网络 感知机的激活函数为阶跃函数，输出结果只有0和1两种，当激活函数为其他非线性函数时则为神经网络。
如下所示，为3层神经网络的实例：
输入层和输出层之间为中间层，又称为隐藏层，其中输入层(第0层)有2个神经元，第一个隐藏层(第1层)有3个神经元，第2个隐藏层(第2层)有2个神经元，输出层(第3层)有2个神经元。
上述激活函数主要作用于中间层。如下表示了激活函数$h()$的效果示例：]]></description>
</item>
<item>
    <title>深度学习:感知机</title>
    <link>https://atuowgo.github.io/deeplearning_01_perceptron/</link>
    <pubDate>Thu, 02 Feb 2023 01:36:54 -0700</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/deeplearning_01_perceptron/</guid>
    <description><![CDATA[感知机 最近陈硕在给我授课，讲深度学习，第一节内容主要是感知机
1. 什么是感知机 感知机是二分类的线性模型，其输入是实例的特征向量，输出的是实例的类别，分别为0和1，属于判别模型。感知机接收多个输入信号，输出一个信号。
下图是一个接收两个输入信号的感知机的例子。
其中$x_{1}、 x_{2}$是输入信号，$y$是输出信号， $w_{1}、 w_{2}$是权重。图中的○称为“神经元”或者“节点”。输入信号被送往神经元时，会被分别乘以固定的权重。神经元会计算传送过来的信号的总和（$w_{1}x_{1} + w_{2}x_{2}$），只有当这个总和超过了某个界限值时，才会输出1。这也称为“神经元被激活” 。这里将这个界限值称为阈值，用符号$\theta$表示。
把上述内容用数学式来表示，就是下面这个式子
2. 感知机模型 感知机是有生物学上的一个启发，我们的大脑可以认为是一个神经网络，这么一个生物的神经网络是由神经元组织起来的，在这个生物的神经网络里边，神经元的一些工作机制就是通过这样一个下面图的结构来运行的。首先接收到一些信号，这些信号通过这些树突组织，树突组织接收到这些信号送到细胞里边的细胞核，这些细胞核对接收到的这些信号（如光、声音）到树突的时候会产生一些微弱的生物电，从而形成一些刺激，那么在细胞核里边对这些收集到的接收到的刺激进行综合的处理，当它的信号达到了一定的阈值之后，它就会被激活，从而产生一个刺激的输出，那么就会形成一个我们大脑接收到的进一步的信号，再通过轴突来输出计算的，这就是我们人脑的一个神经元进行感知的时候大致的一个工作原理。
结合神经元模型有人提出了经典的抽象模型：M-P神经元模型，如下图
而感知机模型是从神经元模型发展过来的。感知机是由两层神经元组成的一个简单模型。只有输出层是M-P 神经元，即只有输出层神经元进行激活函数处理，也称为功能神经元（ functional neuron）；输入层只是接受外界信号（样本属性）并传递给输出层（输入层的神经元个数等于样本的属性数目），而没有激活函数。
(1.1)公式对应
将输入和权重扩展到多维，则式子(1.1)可以表示为
其中激活函数为越阶函数
3. 感知机的应用 感知机可以用来表示简单的逻辑电路，逻辑电路是有两个输入和一个输出的门电路,最简单的例子当然是逻辑电路当中的与或门。与门、或门、非门其实都差不多，都是有两个输入和一个输出。下图为与门的真值表
$x_1$ $x_2$ $y$ 0 0 0 1 0 0 0 1 0 1 1 1 下面考虑用感知机来表示这个与门,需要做的就是确定公式(1.1)成立时能满足图中的真值表的$(w_1,w_2,\theta)$的值
$x_1$ $x_2$ $y$ (1.1)代入 阈值条件 0 0 0 $w_1 * 0 + w_2 * 0 -\theta \lt 0$ $\theta \gt 0$ 0 1 0 $w_1 * 0 + w_2 * 1 -\theta \lt 0$ $\theta \gt w_2$ 1 0 0 $w_1 * 1+ w_2 * 0 -\theta \lt 0$ $\theta \gt w_1$ 1 1 1 $w_1 * 1 + w_2 * 1 -\theta \lt 0$ $\theta \le w_1 + w2$ 很明显该表有值，如$w_1 = w_2 且 \theta = 1.]]></description>
</item>
</channel>
</rss>
