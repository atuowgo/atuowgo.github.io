<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>源码系列 - 分类 - 程序猿二腊</title>
        <link>https://atuowgo.github.io/categories/srccode/</link>
        <description>源码系列 - 分类 - 程序猿二腊</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Thu, 10 Dec 2020 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://atuowgo.github.io/categories/srccode/" rel="self" type="application/rss+xml" /><item>
    <title>Dubbo Provider启动都做了什么</title>
    <link>https://atuowgo.github.io/dubbo_03_provider/</link>
    <pubDate>Thu, 10 Dec 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/dubbo_03_provider/</guid>
    <description><![CDATA[这节介绍Provider的启动过程，既服务发布。
先从Demo看起：
1 2 3 4 5 6 7 8 9 10 11 12 public class ApplicationApi { public static void main(String[] args) throws Exception { ServiceConfig&lt;DemoServiceImpl&gt; service = new ServiceConfig&lt;&gt;(); service.setApplication(new ApplicationConfig(&#34;dubbo-demo-api-provider&#34;)); service.setRegistry(new RegistryConfig(&#34;zookeeper://127.0.0.1:2181&#34;)); service.setInterface(DemoService.class); service.setRef(new DemoServiceImpl()); service.export(); System.in.read(); } } 上面Demo主要做了几件事
新建一个ServiceConfig对象，既服务配置对象 为服务配置设置ApplicationConfig，既服务对应的项目配置 为服务配置设置RegistryConfig，既注册中心配置 指定服务关联的接口DemoService 指定服务关联的实现DemoServiceImpl 调用ServiceConfig的export方法暴露服务 这里重点关注export方法。
在这之前先说下服务暴露的主要流程：一个应用(ApplicationConfig)包含很多服务，一个服务(ServiceConfig)需要服务提供者(ProviderConfig)通过某种协议(ProtocolConfig)将自身的信息注册到注册中心(RegistriesConfig)，这就是服务暴露主要做的事情。
export方法主要围绕上面几个组件展开。
1.设置默认配置 export会先检查当前环境是否准备就绪，主要是检查各组件配置，如ProviderConfig、ProtocolConfig、ApplicationConfig、ModuleConfig、RegistriesConfig、MonitorConfig等。对于某些为空的配置项则会使用默认值，如：
创建ProviderConfig，设置prefix，如果有传则使用原值，没有则新建一个ProviderConfig，并设置prefix为dubbo.provider 创建ProtocolConfig，设置prefix，如果有则使用原值，如果没有设置则新建一个ProtocolConfig，使用默认协议dubbo,并设置prefix为dubbo.protocol. 以上各配置项都继承自AbstractConfig
各配置的实例对象会托管到ConfigMananger，同时被ServiceConfig引用。各实例初始化后的主要值如下：
PS:当id为空时，会设置默认值为default。
2.暴露服务 ServiceConfig准备完配置后就开始进行服务的暴露，主要内容为doExportUrls方法。
该方法分为两个过程：
加载所有的注册中心配置RegistriesConfig 遍历所有的协议配置ProtocolConfig，使用各种协议将服务本身注册到所有的注册中心 2.1 加载注册中心配置 加载ServiceConfig指定的所有注册中心配置对象，并以URL的形式返回。需要提及的一点时，Dubbo中为了增加统一的拦截处理，会把URL的协议改为registry，等拦截处理完后再修改回来。
如开头例子的URL会从
1 zookeeper://127.]]></description>
</item>
<item>
    <title>Dubbo自适应扩展机制</title>
    <link>https://atuowgo.github.io/dubbo_02_extend/</link>
    <pubDate>Mon, 07 Sep 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/dubbo_02_extend/</guid>
    <description><![CDATA[上一节提到过，Dubbo里除了Service和Config层为API，其它各层均为SPI。相比于Java中的SPI仅仅通过接口类名获取所有实现，Dubbo的实现可以通过接口类名和key值来获取一个具体的实现。通过SPI机制，Dubbo实现了面向插件编程，只定义了模块的接口，实现由各插件来完成。
1. 使用方式 1.1 Java SPI 在扩展类的jar包内，放置扩展点配置文件META-INF/service/接口全限定名，内容为：扩展实现类全限定名，多个实现类用换行符分隔。
如下为Mysql中Driver接口的实现：
1 2 3 4 5 6 7 package com.mysql.jdbc; import java.sql.SQLException; public class Driver extends NonRegisteringDriver implements java.sql.Driver { ... } 调用时使用ServiceLoader加载所有的实现并通过循环来找到目标实现类
1 2 3 4 5 6 7 8 9 ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); Iterator&lt;Driver&gt; driversIterator = loadedDrivers.iterator(); try{ while(driversIterator.hasNext()) { driversIterator.next(); } } catch(Throwable t) { // Do nothing } 1.2 Dubbo SPI 拿Dubbo中Protocol接口来说，Protocol的定义如下：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 package org.]]></description>
</item>
<item>
    <title>Dubbo简介</title>
    <link>https://atuowgo.github.io/dubbo_01_introduce/</link>
    <pubDate>Sun, 30 Aug 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/dubbo_01_introduce/</guid>
    <description><![CDATA[这篇是开门贴，内容来自网上。
Dubbo是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的 RPC 实现服务的输出和输入功能，可以和Spring框架无缝集成。Dubbo是一款高性能、轻量级的开源Java RPC框架，它提供了三大核心能力：面向接口的远程方法调用，智能容错和负载均衡，以及服务自动注册和发现。(来自百度百科)
1. 整体架构 Dubbo的整体架构如下：
涉及到的组件包括：
Provider：暴露服务的服务提供方 Consumer：调用远程服务的服务消费方 Registry：服务注册与发现的注册中心 Monitor：统计服务的调用次数和调用时间的监控中心 Container：服务运行容器 调用流程为：
服务容器负责启动，加载，运行服务提供者。服务提供者在启动时，向注册中心注册自己提供的服务。服务消费者在启动时，向注册中心订阅自己所需的服务。注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。(上面内容来自Dubbo官方文档)
2. 使用demo 2.1 Api方式 纯Api方式调用模式如下：
Provider：
1 2 3 4 5 6 7 8 9 10 11 12 public class ApplicationApi { public static void main(String[] args) throws Exception { ServiceConfig&lt;DemoServiceImpl&gt; service = new ServiceConfig&lt;&gt;(); service.setApplication(new ApplicationConfig(&#34;dubbo-demo-api-provider&#34;)); service.setRegistry(new RegistryConfig(&#34;zookeeper://127.0.0.1:2181&#34;)); service.setInterface(DemoService.class); service.setRef(new DemoServiceImpl()); service.export(); System.in.read(); } } Consumer：]]></description>
</item>
<item>
    <title>JVM层面的切面实现 : jvm-sandbox 之 &lt;事件机制&gt;</title>
    <link>https://atuowgo.github.io/sandbox_02_event/</link>
    <pubDate>Sun, 02 Aug 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/sandbox_02_event/</guid>
    <description><![CDATA[这节介绍jvm-sandbox的事件机制，事件机制提供了切面通知的核心功能，内部主要结合asm和观察者模式来实现。jvm-sandbox提供的事件包括：
上面的截图是官网github wiki对事件的介绍，包括提供的事件类型、事件的作用域以及状态机流转。概括的说就是，jvm-sandbox围绕了目标方法这个切面点，提供了多种通知机制。
1.观察者模式 观察者模式的简单示意如下：
Observer(观察者)观察Subject(目标)，Subject在发生变化时通知Observer。具体到这里，模型抽象为:
目标对象为符合条件的类的方法，观察者则为EventListener实现类。一般我们在实现时，
Subject都会有一个Observer列表，以及提供一个添加观察者的方法(add) 用户通过调用Subject的add方法，触发观察，把Observer添加到Observer列表中 Subject在事件发生时，遍历Observer列表，通知到Observer 以此达到目的。下面来看jvm-sandbox如何实现该过程。
2. 触发观察DefaultModuleEventWatcher.watch 从官网的例子开始
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * 一个损坏的钟实现 */ static class BrokenClock extends Clock { @Override void checkState() { throw new IllegalStateException(); } @Override void delay() throws InterruptedException { Thread.sleep(10000L); } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /** * 修复损坏的钟模块 */ @Information(id = &#34;broken-clock-tinker&#34;) public class BrokenClockTinkerModule implements Module { @Resource private ModuleEventWatcher moduleEventWatcher; @Http(&#34;/repairCheckState&#34;) public void repairCheckState() { moduleEventWatcher.]]></description>
</item>
<item>
    <title>JVM层面的切面实现 : jvm-sandbox 之 &lt;应用启动&gt;</title>
    <link>https://atuowgo.github.io/sandbox_01_start/</link>
    <pubDate>Fri, 24 Jul 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/sandbox_01_start/</guid>
    <description><![CDATA[1. 启动 sandbox源码各模块的结构如下:
sandbox安装后的目录结构如下:
按照官方的教程，启动命令如下：
1 ./sandbox.sh -p 2343 查看对应的脚本，函数定义如下：
翻译过来就是执行如下命令
1 java -Xms128M -Xmx128M -Xnoclassgc -ea -jar /xxx/sandbox-core.jar 2342 /xxx/sandbox-agent.jar home=/xxx;token=xxx;server.ip=xxx;service.port=xxx;namespace=xxx 按顺序将参数定义为
targetJvmPid:目标应用pid agentJarPath:附加的agent包路径 cfg:agent-main的参数,包括home路径;该次唯一标识toke;ip端口以及命名空间 查看sandbox-core模块的pom.xml
1 2 3 4 5 &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.alibaba.jvm.sandbox.core.CoreLauncher&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; 找到启动类 com.alibaba.jvm.sandbox.core.CoreLauncher
该类存在一个main方法，接收3个参数，并实例化一个CoreLauncher实例，它在构造方法中完成agent的attach动作.
1 2 3 4 5 6 7 public CoreLauncher(final String targetJvmPid, final String agentJarPath, final String token) throws Exception { attachAgent(targetJvmPid, agentJarPath, token); } 主要是以agent-main的方式执行。
去到sandbox-agent模块的pom.xml，有如下配置
1 2 3 4 5 6 7 8 &lt;archive&gt; &lt;manifestEntries&gt; &lt;Premain-Class&gt;com.]]></description>
</item>
<item>
    <title>RocketMQ消息的存储消费模型</title>
    <link>https://atuowgo.github.io/rocketmq_09_consumequeue/</link>
    <pubDate>Mon, 18 May 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/rocketmq_09_consumequeue/</guid>
    <description><![CDATA[这节介绍RocketMQ中消息的存储模型：CommitLog和ConsumeQueue。CommitLog用于存储Producer的消息，ConsumeQueue接受CommitLog分配的消息，等待Consumer消费。
1. 存储消费模型 RocketMQ采用了单一的日志文件，即把同1台机器上面所有topic的所有queue消息都存放在一个文件里面，以避免随机的磁盘写入。其存储结构如下：
Broker会把所有消息都存在一个单一的CommitLog文件里面，然后由后台线程异步的同步到ConsumeQueue中，再由Consumer进行消费。
该模型下，ConsumeQueue中并不需要存储消息的内容，而是存储消息在CommitLog中的offset。也就是说，ConsumeQueue其实是CommitLog的一个索引文件。即CommitLog的写入是顺序的，而读取需要支持随机读的，支持该特性的实现就是上一节提到的MappedFileQueue。
2. CommitLog CommitLog的很多操作都基于MappedFileQueue来操作，在MappedFileQueue上增加了CRUD操作。CommitLog的调用过程如下：
初始化时会初始化相关的组件，包括MappedFileQueue、FlushCommitLogService、AppendMessageCallback等。
MappedFileQueue主要设定了文件的路径位置和每个MappedFile的大小(1024 * 1024 * 1024 = 1G)。
FlushCommitLogService的实现类有3个GroupCommitService、FlushRealTimeService和CommitRealTimeService。前两个用于刷盘，如果开启了异步刷盘，则使用GroupCommitService，该实现会将刷盘请求缓存起来，到时间后再执行刷盘；FlushRealTimeService，该实现直接定时刷固定页的数据，如果启用了堆外缓冲区则使用GroupCommitService，否则使用FlushRealTimeService。后面的CommitRealTimeService则用于将堆外缓冲区中的数据写到FileChannel中(如果启用了堆外缓冲，调用MappedFileQueue的commit方法)。
AppendMessageCallback主要用于实现具体消息写入ByteBuf的方式。
此外，还缓存了每个topic下每个queue对应的逻辑偏移量
1 private HashMap&lt;String/* topic-queueid */, Long/* offset */&gt; topicQueueTable = new HashMap&lt;String, Long&gt;(1024); 加载主要是调用MappedFileQueue的load方法，最后启动FlushCommitLogService服务。
同时，CommitLog还通过ReputMessageService，将消息offset放到了ConsumeQueue以及IndexService，ReputMessageService的调用在DefaultMessage里。
2.1. 追加消息 追加消息的大致过程如下，其中写入消息时会加锁，从而保证每个CommitLog文件的写都是顺序写入的。
其中AppendMessageCallback主要完成了对消息写入ByteBuff的处理，包括单笔和批量消息。这里主要介绍单笔消息的写入：计算一条消息每一部分的大小和内容，然后写入到byteBuffer中，byteBuffer是MappedFile slice后的内容，写入每个部分如下：
大部分内容通过字段便能够看出具体的意思，这里不再赘述，只对一些内容进行介绍。
计算消息的全局ID：MessageId，MessageId由存储的broker机器的ip地址+commitLog中的起始物理offset组成。MessageId会带在返回结果PutMessageResult中返回。 写入成功后会将topic-queue的逻辑偏移量+1(逻辑偏移量表示消息数量的大小，一条消息进来下标就会长1) 写入之前会计算消息的大小，如果超过设定的单条消息的大小(默认512K)则拒绝写入 写入之前会判断当前文件剩余内容是否足够，如果不够则会插入一条空白消息，将剩余部分填满，返回文件已满，上层判断文件满了，会重新生成一个文件，将消息再次写入.空白消息写入的内容为totalSize+magicCode+0值,其中正常消息的magicCode为-626843481，空白消息为-875286124 2.2. 读取消息 根据所给的起始offset(物理偏移量),确定该offset所在MappedFile后，获取从offset开始到MappedFile当前可读的所有内容。
MappedFile里会维护一个fileFromOffset属性，标记当前文件存储消息的起始偏移量，该值也是文件的文件名，默认值每个文件为1G=1073741824,即每个文件名的步长为1073741824。对于给定的offset，只要找到符合fileFromOffset &lt;= offset &lt; fileFromOffset + mappedFileSize(1G)的MappedFile即可。找到目标MappedFile后，通过offset%mappedFileSize可以计算出所给的起始offset在当前文件中的位置。
获得ByteBuff内容后，可以调用checkMessageAndReturnSize方法，从ByteBuff中还原消息，返回DispatchRequest(不含消息body内容)。
前面提到，ReputMessageService会从CommitLog中读取消息，然后用于构建ConsumeQueue和IndexFile，这里传送的内容就是DispatchRequest。DefaultMessageStore会定时从CommitLog中读取消息，并将解析出的DispatchRequest对象传给ConsumeQueue和IndexService。
2.3. 删除消息 调用MappedFileQueue，按照offset或者过期时间(默认72小时)删除物理文件
3. ConsumeQueue 代表一个topic下一个queue的消费队列。ConsumeQueue底层基于MappedFileQueue，存储每个topic-queueId下对应消息的offset，但消息被实际消费时，再通过offset从CommitLog中查找具体的消息内容。
ConsumeQueue的调用过程和提供方法同CommitLog类似，也是初始化时从指定的路径下重构MappedFileQueue，并提供了CRUD方法来操作文件。
ConsumeQueue以路径的形式来分割每个topic下queue的存储，每个q对应的数据存储路径为
/DirToStore/topic/queueId/ 每次写入的数据格式(数据单元)为：
1 物理偏移量(long/8)|消息大小(int/4)|逻辑偏移量(long/8) = 20 ConsumeQueue初始化MappedFile的默认大小为 30000 * 20，即每个文件存30万条记录，没个文件名的步长为600000，消费时只要每次读取20个字节便能查找到每个消息的offset。]]></description>
</item>
<item>
    <title>RocketMQ文件存储的基础:MappedFile和MappedFileQueue</title>
    <link>https://atuowgo.github.io/rocketmq_08_mappedfile/</link>
    <pubDate>Thu, 14 May 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/rocketmq_08_mappedfile/</guid>
    <description><![CDATA[RocketMQ文件存储的基础:MappedFile和MappedFileQueue RocketMQ中的MappedFile类对应一个文件的mmap映射,是RocketMQ实现高效存储的基础。本文先介绍零拷贝的相关背景，再介绍RocketMQ中mmap的应用，为后面介绍MessageStore做过渡。
1. 零拷贝 零拷贝（zero copy）指的是当拷贝发生时，CPU并不参与实际的拷贝过程。CPU可以切换到其他线程，数据的拷贝过程异步进行，异步过程通常要由硬件DMA实现。常用的零拷贝有 mmap 和 sendFile。
采用传统的读写操作将磁盘中的数据发送到网络中，通常经历2次用户态/内核态的切换，并且读和写操作CPU分别要参与一次拷贝过程。
我们知道，操作系统设置了用户态和内核态两种状态，用户态想要获取系统资源(例如访问硬盘), 必须通过系统调用进入到内核态, 由内核态获取到系统资源,再切换回用户态返回应用程序。同时，操作系统在内核态中也增加了一个&quot;内核缓冲区&quot;(kernel buffer)，读取数据时并不是直接把数据读取到应用程序的缓存区(app buffer), 而是先读取到内核缓冲区, 再由内核缓冲区复制到应用程序的缓存区。
传统的IO读写过程，数据会在用户态和内核态之间发送多次复制和多次上下文切换，如下：
主要流程为：
硬盘拷贝到内核缓冲区(DMA COPY) 内核缓冲区拷贝到应用程序缓冲区(CPU COPY) 应用程序缓冲区拷贝到socket缓冲区(CPU COPY) socket buf拷贝到网卡的buf(DMA COPY) 即一次读写过程涉及到2次cpu copy, 还有4次的上下文切换。
很明显,第2次和第3次的copy只是把数据复制到app buffer又原封不动的复制回来, 为此带来了两次的cpu copy和两次上下文切换, 是完全没有必要的。
1.1 PageCache pagecache是文件系统层级的缓存，从磁盘里读取的内容是存储到这里，这样程序读取磁盘内容就会非常快。page cache的大小为一页，通常为4K。
Java里的写操作都是写到PageCache里便认为逻辑落盘成功，后续操作是通过操作系统把它刷到磁盘文件上。
1.2. sendfile linux内核2.1开始引入一个叫sendFile系统调用
1 ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 这个系统调用可以在内核态内把数据从内核缓冲区直接复制到套接字(SOCKET)缓冲区内, 从而可以减少上下文的切换和不必要数据的复制,如下：
涉及到数据拷贝变成:
硬盘拷贝到内核缓冲区(DMA COPY) 内核缓冲区拷贝到socket缓冲区(CPU COPY) socket缓冲区拷贝到网卡的buf(DMA COPY) sendfile只能将数据从文件传递到套接字上，反之则不行。 使用sendfile不仅减少了数据拷贝的次数，还减少了上下文切换。
Java类库通过java.nio.channels.FileChannel的transgerTo方法支持零拷贝。而在Netty中也通过在FileRegion中包装了NIO的FileChannel.transferTo()方法实现了零拷贝。RocketMQ在涉及到网络传输的地方也使用了该方法。
FileChannel本身不是基于零拷贝实现的，而是是基于块来实现的。FileChannel配合着ByteBuffer，将读写的数据缓存到内存中，然后以批量/缓存的方式read/write，省去了非批量操作时的重复中间操作，操纵大文件时可以显著提高效率。FileChannel的write方法将数据写入PageCache后就认为落盘了，最终还是要操作系统完成PageCache到磁盘的最终写入。FileChannel的force方法则是用于通知操作系统进行及时的刷盘。
1.3. mmap mmap也是零拷贝实现的一种，它利用共享内存空间的方式, 把文件映射到用户空间里的虚拟内存，省去了从内核缓冲区复制到用户空间的过程。]]></description>
</item>
<item>
    <title>RocketMQ的心脏:Broker</title>
    <link>https://atuowgo.github.io/rocketmq_07_broker/</link>
    <pubDate>Mon, 04 May 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/rocketmq_07_broker/</guid>
    <description><![CDATA[RocketMQ的心脏:Broker 这节介绍下RocketMQ中最后的一个部分，也是内容较多的一部分：Broker。
Broker的启动同其他几个组件一样，从XXXStartup(BrokerStartup)类的main方法开始，首先加载对应的配置文件XXXConfig(BrokerConfig、NettyServerConfig、NettyClientConfig、MessageStoreConfig)，然后实例化XXXController(BrokerController)，接着调用Controller的initialize方法，最后注册ShutdownHook。
Broker在构造方法中，会进行如下的实例化动作：
包括：
配置类：BrokerConfig、NettyServerConfig、NettyClientConfig、MessageStoreConfig 管理类：ConsumerOffsetManager(偏移量管理)、TopicConfigManager(topic配置管理)、ConsumerManager(消费者管理)、ConsumerFilterManager(消费者过滤管理)、ProducerManager(生产者管理)、SubscriptionGroupManager(订阅组管理)、FilterServerManager(服务端过滤管理)、BrokerStatsManager(broker状态管理) 服务类：PullMessageProcessor(拉取消息处理器)、PullRequestHoldService(拉取请求缓存服务)、ClientHousekeepingService(客户端长连接服务)、SlaveSynchronize(slave同步) 监听类：NotifyMessageArrivingListener(通知消息到达监听器)、DefaultConsumerIdsChangeListener(客户端id改变监听器) 工具类：Broker2Client、BrokerOuterAPI 线程队列类：发送线程、拉取线程、查询线程、客户端管理线程、消费者管理线程、心跳线程、事务线程 Broker的初始化和启动过程如下：
初始化的步骤为：
加载配置文件,由TopicConfigManager处理:存储每个topic的配置信息；ConsumerOffsetManager:缓存所有topic@group对应的queue的偏移量；SubscriptionGroupManager:存储每个group的配置信息；ConsumerFilterManager:Topic的过滤表达式信息 加载消息存储内容,由MessageStore处理 注册请求处理器，由NettyRequestProcesser处理，根据RequestCode，分发远程请求给对应的Processer，包括SendMessageProcessor、QueryMessageProcessor、ClientManageProcessor、ConsumerManageProcessor、AdminBrokerProcessor 每隔一天记录broker状态，BrokerStats会打印当天接受/处理的消息总数 每一段时间（默认5s）记录消费者消费的偏移量，ConsumerOffsetManager会将缓存topic下每个q对应的消费偏移量进行持久化存储为json 每10s记录消费者过滤情况，ConsumerFilterManager会将缓存topic的过滤表达式持久化存储为json 每3min“保护”broker，BrokerStatsManager会每秒/分钟/小时记录各字表的次数和消费消息的大小，如果记录的客户端消费失败字节数大于配置的参数，会将该topic设置为不可消费 每1s打印打印消息队列水位线，包括发送Queue数量、拉取Queue数量、查询Queue数量 每1分钟打印处理的消息量 同步namesrv地址列表，由BrokerOutterAPI处理，如果指定了namesrv地址，则使用该列表；否则从配置的网络路径上同步 (slave)每1min同步slave，SlaveSynchronize会通过BrokerOutterApi向master请求数据同步 (master)每1min打印slave比master落后的消息数，MessageStore会打印最大的消费偏移量-已经同步给slave的偏移量 初始化事务服务 启动步骤包括：
启动消息存储服务MessageStore 启动Netty服务RemotingServer 启动文件监听服务FileWatchService，监听tls证书，在发生变化时重新加载 启动API服务BrokerOuterAPI 启动缓存pull请求处理服务PullRequestHoldService，对每个topic下的q的pull请求，定时再次执行 启动客户端连接服务ClientHouseKeepingService，每10s清除超时没有消息的客户端连接 启动服务端过滤服务FilterServerManager，每30s执行本地脚本 注册broker，后面每一段时间(不超过1分钟)注册broker，首次启动会向所有namesrv同步broker信息和所持有的topic信息，后面每隔一段时间同步一次 启动broker状态管理BrokerStatsManager 启动请求过期清理任务BrokerFastFailure，如果开启了快速失效的配置，会定时清理缓存中的过期请求 启动事务消息检查服务TransactionalMessageCheckService 下面将介绍其中几块内容。
1. SendMessageProcessor 消息发送处理器，主要处理Producer发送的消息和Consumer的重试消息。
1.1. 处理发送消息请求 主要过程为：
解析请求头，得到SendMessageRequestHeader 构建上下文对象，SendMessageContext 执行前置hook，调用SendMessageHook的sendMessageBefore方法 执行核心处理逻辑，分为单消息和批量消息的处理，主要是先进行前置检查，如判断MesageStore是否已经启动、Queue是否正确等，然后将请求内容包装为Broker内部处理的形式，交由MessageStore处理放入CommitLog中。其中单消息包装为MessageExtBrokerInner、批量消息包装为MessageExtBatch 执行后置hook，调用SendMessageHook的sendMessageAfter方法 1.2. 处理Consumer的重试消息 在介绍Consumer消费消息过程时提到过，在Push模式下，如果消息消费失败，可以将消息重新返回Consumer实例的内存缓存队里中等待消费，也可以由Consumer模拟Producer角色，将消息发送到Broker，等待再次消费。
主要过程为：
解析请求头，得到ConsumerSendMsgBackRequestHeader 如果原消息Id不为空，执行消费后置hook，调用ConsumeMessageHook.consumeMessageAfter 检查前置判断条件，包括所在Group是否存在、是否有权限等 默认放到Retry重试队列中，分配新的topic，格式为 ``%RETRY%+Group```` 检查重试队列配置，包括重试队列配置是否为空、重置队列是否可写等 根据原消息的偏移量offset在CommitLog中查找原消息内容(MessageExt格式) 判断该消息的重试次数是否已经超过设定的最大值(默认16次)，如果是，则将放到DLQ死信队列中，将topic格式更新为``%DLQ%+Group```` 将原消息重新包装为MessageExtBrokerInner对象，并调用MessageStore放到CommitLog中 2. PullMessageProcessor 消息拉取处理器，在介绍消费者消费消息过程时提到过，RocketMQ内部都是通过Pull方式从Broker拉取消息的，Push模式是通过包装Pull方式，由RocketMQ定时发起，并自动处理offset、消息重试动作。
PullMessageProcessor主要的工作是根据客户端提供的offset，从ConsumeQueue中获取到该topic-queueId在CommitLog中的起始位置，每次读取消息都会从ConsumeQueue中尽可能多的读取消息，并计算出客户端下次的offset,把结果返回。如果提供的offset过大，会将请求暂缓，在超时时间内或者ConsumeQueue的数据符合后，再次处理请求。
具体的处理流程如下：
上面过程比较清晰，大多是前置检查，主要看&quot;判断GetMessageResult&quot;的过程，如下：
该过程主要是设置返回结果RemotingCommand的值，包括：]]></description>
</item>
<item>
    <title>轻量级注册中心：RocketMQ NameServer</title>
    <link>https://atuowgo.github.io/rocketmq_06_nameserver/</link>
    <pubDate>Wed, 15 Apr 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/rocketmq_06_nameserver/</guid>
    <description><![CDATA[NameServer实现比较简单，主要包括配置管理、路由管理两块。其主要功能是为整个MQ集群提供服务协调与治理，具体就是记录维护Topic、Broker的信息，及监控Broker的运行状态，为client提供路由能力。NameServer是一个几乎无状态节点，可集群部署，节点之间无任何信息同步。
NameServer为生产者和消费者提供Meta数据，以确定消息该发往哪个Broker或者该从哪个Broker拉取消息。有了Meta数据后，生产者和消费者就可以直接和 Broker交互了。这种点对点的交互方式最大限度降低了消息传递的中间环节，缩短了链路耗时。
1. 启动 NameServer的启动过程如下：
加载本地的备份文件，载入k-v对，有KVConfigManager持有，格式如下 1 HashMap&lt;String/* Namespace */, HashMap&lt;String/* Key */, String/* Value */&gt;&gt; configTable = new HashMap&lt;String, HashMap&lt;String, String&gt;&gt;(); 在有值发生变化的时候就会执行一次持久化 2. 初始化NettyRemotingServer，传入BrokerHousekeepingService，会在客户端连接发生变化的时候进行路由管理的变更 3. 初始化线程池，用于NettyRemotingServer分发请求，详见上一节关于Netty的介绍 4. 注册请求处理器，NameServer的实现为DefaultRequestProcessor，提供了包括PUT_KV_CONFIG、GET_KV_CONFIG等管理功能的CRUD远程调用入口 5. 启动定时器，每5秒扫描超时的broker连接并移除 6. 启动定时器，每10分钟打印一次kv对信息
2. 服务管理 NameServer的对外服务入口由DefaultRequestProcessor提供，主要包括配置管理和路由管理，分别由KVConfigManager和RouteInfoManager实现。
2.1. KVConfigManager 比较简单，中规中矩的一个类，内部有一个configTable按照命名空间管理配置键值对，除了put和get操作外，还有持久化为json以及加载持久化文件的操作。
2.2. RouteInfoManager RouteInfoManager主要用来管理Broker及Topic对应的路由信息，类关系如下：
包括：
Topic对应的Queue列表 Queue所在的Broker，以及Queue对应的属性 Broker对应的集群信息、集群中的实例地址信息 每个Broker实例对应的存活信息，包括最后更新时间、版本号、Netty io连接、HA地址 集群对应的Broker列表 主要用于管理Broker和Topic的路由信息。
2.2.1. 注册Broker(registerBroker) 方法定义如下：
1 2 3 4 5 6 7 8 9 public RegisterBrokerResult registerBroker( /*集群名*/final String clusterName, /*broker地址*/final String brokerAddr, /*broker名*/final String brokerName, /*brokerId*/final long brokerId, /*ha地址*/final String haServerAddr, /*broker上对应的topic配置项*/final TopicConfigSerializeWrapper topicConfigWrapper, /*需要过滤的server列表*/final List&lt;String&gt; filterServerList, /*broker对应的channel*/final Channel channel) 过程如下：]]></description>
</item>
<item>
    <title>Netty-RocketMQ底层通信的利器</title>
    <link>https://atuowgo.github.io/rocketmq_05_netty/</link>
    <pubDate>Mon, 30 Mar 2020 00:00:00 &#43;0000</pubDate>
    <author>二腊</author>
    <guid>https://atuowgo.github.io/rocketmq_05_netty/</guid>
    <description><![CDATA[这节介绍RocketMQ底层通信的原理
在之前的内容中有介绍过RocketMQ底层用了Netty来进行通信，下图为RocketMQ通信的大致过程，主要分为Server端和Client端。
客户端通过invokeSyncImpl、invokeAsyncImpl、invokeOnewayImpl这几个方法同服务端交互。
1. NettyRemotingServer Server启动主要是初始化ServerBootstrap，主要配置如下：
设置tcp的参数，包括SO_BACKLOG、SO_REUSEADDR、SO_KEEPALIVE、TCP_NODELAY等。 设置pipeline处理链，包括编码、解码、空闲处理、连接管理、请求分发。 启动完ServerBootstrap后会启动一个定时器，每3秒清除超时的请求。
这里介绍下面几个处理器：
NettyEncoder NettyDecoder NettyConnectManageHandler NettyServerHandler 1.1. NettyEncoder NettyEncoder继承自LengthFieldBasedFrameDecoder，主要有用于解码入站数据流，并将数据流解码为RemotingCommand对象。
LengthFieldBasedFrameDecoder（自定义长度解码器）的构造器，涉及5个参数，都与长度域（数据包中的长度字段）相关，具体介绍如下：
maxFrameLength：发送的数据包最大长度；
lengthFieldOffset：长度域偏移量，指的是长度域位于整个数据包字节数组中的下标；
lengthFieldLength：长度域的自己的字节数长度。
lengthAdjustment：长度域的偏移量矫正。 如果长度域的值，除了包含有效数据域的长度外，还包含了其他域（如长度域自身）长度，那么，就需要进行矫正。矫正的值为：包长 - 长度域的值 – 长度域偏移 – 长度域长。
initialBytesToStrip：丢弃的起始字节数。丢弃处于有效数据前面的字节数量。比如前面有4个节点的长度域，则它的值为4。
以NettyEncoder为例，器构造构造方法为
1 2 3 public NettyDecoder() { super(FRAME_MAX_LENGTH, 0, 4, 0, 4); } 即数据流中前4个字节的值表示有效数据域的长度，除开前4个字节外的内容都是有效数据域的内容，不存在偏移量。
接收到数据域的内容后，便会调用RemotingCommand.decode方法，将数据流转为RemotingCommand对象。
RemotingCommand对象分为Header部分和Body部分。Header部分包括固定的一组字段，已经长度不定的扩展字段；Body部分为byte[]，不进行具体的细分。
数据域的解析过程同上面的类似，数据域中前4个自己为Header域的长度，取到Header长度后便能计算出Body长度，从而进行读取。RemotingCommand的内容如下：
根据serializerType的不同，Header的编码会分为Json或者二进制的方式。
1.2. NettyDecoder NettyEncoder的反过程，将RemotingCommand对象序列化为ByteBuffer对象。根据serializerType的不同，Header会编码为JSON或者二进制。
1.3. NettyConnectManageHandler NettyConnectManageHandler继承自ChannelDuplexHandler，用于监听pipeline中入站/出站的事件，主要进行日志记录。
1.4. NettyServerHandler NettyServerHandler继承自SimpleChannelInboundHandler，重写了channelRead0方法，在里面调用了父类NettyRemotingAbstract的processMessageReceived方法，如下：
该方法定义了请求和响应的处理过程。
1.processRequestCommand
处理请求过程，先根据RemotingCommand中的code值判断当前请求是否能够处理，如果不能处理则直接响应不支持。如果可以支持，则会找到对应的处理器，新起线程来处理当前请求。需要说明的是，NettyRemotingServer内部维护这一个processorTable，表示该server可以处理的command，对应的Processor以及对应的线程池。
1 2 protected final HashMap&lt;Integer/* request code */, Pair&lt;NettyRequestProcessor, ExecutorService&gt;&gt; processorTable = new HashMap&lt;Integer, Pair&lt;NettyRequestProcessor, ExecutorService&gt;&gt;(64); Processor的定义如下，对于具体的command，会由对应的Processor来处理]]></description>
</item>
</channel>
</rss>
